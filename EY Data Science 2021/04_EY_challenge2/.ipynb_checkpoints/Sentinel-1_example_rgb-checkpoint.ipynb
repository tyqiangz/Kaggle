{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Sentinel-1 Radar imagery <img align=\"right\" src=\"../Supplementary_data/EY_logo.png\" style=\"margin:0px 50px\">\n",
    "\n",
    "\n",
    "Radar imagery can be used as a complementary dataset for Challenge 2. \n",
    "\n",
    "Radar is an active measurement technique, illuminating the Earth's surface and detecting the light scattering effects, rather than relying on the sun to illuminate the Earth, such as in passive, optical techniques. Synthetic Aperture Radar (SAR) is a form of radar often used on satellites, where the motion of the satellite over the landscape allows for higher spatial resolution measurements to be obtained. SAR measurements can penetrate cloud and smoke cover, making it particularly useful for assessing bushfires, and can also operate at night.\n",
    "\n",
    "The European Space Agency (ESA)'s Sentinel-1 mission consists of two satellites: Sentinel-1A (launched in 2014) and Sentinel-1B (launched in 2016). Each houses an SAR instrument on board. These instruments provide single C-band 1 dB radiometric accuracy with a central frequency at 5.405 GHz. \n",
    "\n",
    "Together, the Sentinel-1 satellites provide:\n",
    "- global coverage \n",
    "- frequent imaging (the satellites have a frequent revisit time of 6 days.)\n",
    "- different polarizations to capture different geospatial properties (e.g., VV, VH, as explained further below)\n",
    "- and can detect millimetre changes in elevation.\n",
    "\n",
    "This notebook explores the Sentinel-1 datasets. You can use these in your analysis to assist with the fire progression prediction in Challenge 2. \n",
    "\n",
    "As an example, this notebook shows what information we can obtain from the Sentinel-1 Radio Terrain Correction (RTC) gamma-0 data product `s1_rtc`.  The RTC gamma-0 data are corrected for variations caused by changing observation geometries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from datacube import Datacube\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../scripts\")\n",
    "import pandas as pd\n",
    "from dea_plotting import display_map\n",
    "from dea_plotting import rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ### Set up distributed computing\n",
    "\n",
    "Calculating the maximum value for each pixel in an image can be computationally expensive. However, it is possible to reduce the computation time by parallelising the process through Dask. Access to Dask is provided in the Azure environment. \n",
    "\n",
    "For more information about using Dask, refer to the Parallel processing with Dask notebook.\n",
    "\n",
    "To set up distributed computing with Dask, you need to first set up a Dask client using the function below: -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentinel-1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a datacube object\n",
    "dc = Datacube(app=\"Sentinel-1 example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate the Sentinel-1 RTC product dataset\n",
    "senti_datasets = dc.find_datasets(product='s1_rtc')\n",
    "senti_datasets = sorted(senti_datasets, \n",
    "                        key = lambda ds: (ds.center_time, ds.id))\n",
    "\n",
    "# check available information for a random dataset\n",
    "sample = senti_datasets[100]\n",
    "print(sample, '\\n')\n",
    "print('sample_0 metadata_label: ', sample.metadata_doc['label'])\n",
    "print(f'\\nNumber of s1_rtc datasets: {len(senti_datasets)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read other infomation in the metadata\n",
    "sample.metadata_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the metadata to match your training data, e.g., by coordinates and datetime properties. This is part of the challenge! Here we load a random single radar image to learn about its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Sentinel-1 backscatter data\n",
    "s1 = dc.load(product='s1_rtc', \n",
    "             id='e320bae8-996e-56ea-ae68-0efbefa47e9a', \n",
    "             output_crs='epsg:4326', \n",
    "             resolution=(-0.0002,0.0002))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'e320bae8-996e-56ea-ae68-0efbefa47e9a'\n",
    "selected_id = [ds for ds in senti_datasets if ds.metadata_doc['id'] == id]\n",
    "\n",
    "# extract latitude, longtitude, and datetime information \n",
    "lat_dict = selected_id[0].metadata_doc['extent']['lat']\n",
    "lon_dict = selected_id[0].metadata_doc['extent']['lon']\n",
    "dt_string = selected_id[0].metadata_doc['properties']['datetime']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = (lat_dict['begin'], lat_dict['end'])\n",
    "longitude = (lon_dict['begin'], lon_dict['end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  View the selected location interactively\n",
    "The next cell will display the selected area on an interactive map. Feel free to zoom in and out to get a better understanding of the area you’ll be analysing. Clicking on any point of the map will reveal the latitude and longitude coordinates of that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display_map(x=longitude, y=latitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise loaded data in different channels\n",
    "\n",
    "Sentinel-1 backscatter data has two measurements, VV and VH, which correspond to the polarisation of the light sent and received by the satellite. VV refers to the satellite sending out vertically-polarised light and receiving vertically-polarised light back, whereas VH refers to the satellite sending out vertically-polarised light and receiving horizontally-polarised light back. These two measurement bands can tell us different information about the area we’re studying.\n",
    "\n",
    "The Co-polarisation (VV) channel is indicative of surface scattering such as soil, rock and corner relections from buildings. Relatively smooth surfaces such as water and bare grounds have very little backscatter and will appear black in the image.\n",
    "\n",
    "Cross-polarisation (VH) channel is indicative of volume scattering. It can be used to separte vegetated areas and non-vegetated areas. Water, bare soil and rock will also appear black in the VH images. Vegetated areas have larger volume scattering and will appear brighter.\n",
    "\n",
    "The ratio of the VV and VH channel is used to highlight the similarity and discrepancy of the two channels. The ratio image provides a higher contrast for areas where the surface scattering and volume scattering act differently. For example, vegetated areas will have a high VH/VV value, whereas bare grounds will have a low VH/VV value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two VH/VV and VV/VH ratio channels\n",
    "s1[\"vv_vh\"] = s1.vv/s1.vh\n",
    "s1[\"vh_vv\"] = s1.vh/s1.vv\n",
    "\n",
    "# add normalised channels for making RGB images below\n",
    "s1[\"vv_r\"] = s1.vv/s1.vv.mean()\n",
    "s1[\"vh_g\"] = s1.vh/s1.vh.mean()\n",
    "s1[\"vhvv_b\"] = s1.vh_vv/s1.vh_vv.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the VH observation\n",
    "s1.vh_g.plot(cmap=\"viridis\", robust=True, figsize=(8, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the VV observation\n",
    "s1.vv_r.plot(cmap=\"viridis\", robust=True, figsize=(8, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the VH/VH ration image\n",
    "s1.vh_vv.plot(cmap=\"viridis\", robust=True, figsize=(8, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a false-color composite image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1\n",
    "s1[[\"vv_r\", \"vh_g\", \"vhvv_b\"]].isel(time=0).to_array().plot.imshow(robust=True, figsize=(6, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2\n",
    "# normalise the datacube so that RGB channels are in a similar range\n",
    "red_arr = xr.DataArray(s1.vv) / np.nanmean(xr.DataArray(s1.vv))\n",
    "green_arr = xr.DataArray(s1.vh) / np.nanmean(xr.DataArray(s1.vh))\n",
    "blue_arr = xr.DataArray(s1.vh_vv) / np.nanmean(xr.DataArray(s1.vh_vv))\n",
    "composite_arr = xr.Dataset({'red': red_arr,\n",
    "                           'green': green_arr,\n",
    "                           'blue': blue_arr})\n",
    "rgb(composite_arr, \n",
    "    bands=['red', 'green', 'blue'], \n",
    "    size=10, index=0,\n",
    "    percentile_stretch = [0.05, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The red colors in the RGB images highlight regions of bare soil or rock (high VV values, low VH and VH/VV values).  The blue areas are low VV and high VH values, indicative of vegetated regions. The dark areas could indicate water. These images can help trace the effect of fire before and after a disaster.\n",
    "\n",
    "Other examples about using the Sentinel-1 data, including making use of\n",
    "the parallel-computing library `Dask` can be found here:\n",
    "https://docs.dea.ga.gov.au/notebooks/Real_world_examples/Shipping_lane_identification.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Additional information\n",
    "\n",
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** If you need assistance, please review the FAQ section and support options on the [EY Data Science Platform](https://datascience.ey.com/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
