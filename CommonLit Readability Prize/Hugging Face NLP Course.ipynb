{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d0e2b6",
   "metadata": {},
   "source": [
    "# Transformers <a name=\"top\"></a>\n",
    "\n",
    "https://huggingface.co/course/chapter0?fw=pt\n",
    "\n",
    "**Table of contents**\n",
    "- [Pipeline](#pipeline)\n",
    "    - [Sentiment Analysis](#sentiment-analysis)\n",
    "    - [Zero shot classification](#zero-shot-classification)\n",
    "    - [Text generation](#text-generation)\n",
    "    - [Mask filling](#mask-filling)\n",
    "    - [Named Entity Recognition](#Named-entity-recognition)\n",
    "    - [Question Answering](#question-answering)\n",
    "    - [Summarisation](#summarisation)\n",
    "    - [Translation](#translation)\n",
    "- [Components of `pipeline`](#components)\n",
    "    - [Tokenizer](#tokenizer)\n",
    "        - [Encoding](#encoding)\n",
    "        - [Decoding](#decoding)\n",
    "        - [Padding](#padding)\n",
    "        - [Attention mask](#attention-mask)\n",
    "        - [Truncation](#truncation)\n",
    "        - [Token type id](#token-type-id)\n",
    "    - [Model](#model)\n",
    "    - [Postprocessing the output](#postprocessing)\n",
    "- [Finetuning](#finetuning)\n",
    "    - [MRPC dataset](#mrpc-dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3172572",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "A very convenient high level function to get nlp tasks done quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57dd05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1e0830",
   "metadata": {},
   "source": [
    "The first parameter of `pipeline` is `task`, we can input any common task e.g. `sentiment-analysis`, `text-generation` and get a function that performs that nlp task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc4775d",
   "metadata": {},
   "source": [
    "### Sentiment Analysis <a name=\"sentiment-analysis\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc42438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047733306885},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier([\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7042c3",
   "metadata": {},
   "source": [
    "### Zero shot classification <a name=\"zero-shot-classification\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38ce537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a course about the Transformers library',\n",
       " 'labels': ['education', 'business', 'politics'],\n",
       " 'scores': [0.8445960879325867, 0.11197630316019058, 0.04342757537961006]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"This is a course about the Transformers library\",\n",
    "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91867d4",
   "metadata": {},
   "source": [
    "### Text generation <a name=\"text-generation\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e80e6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to apply your data analysis skills to understanding, analyzing and applying the RNNs. We will examine various techniques which determine the optimal value of a RNN. Our target audience for the course is those individuals'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"In this course, we will teach you how to\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da071b0",
   "metadata": {},
   "source": [
    "We can specify the model and additional parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b6c26d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'In this course, we will teach you how to use a computer program to quickly copy and paste files stored between the systems. This is an overview of'},\n",
       " {'generated_text': 'In this course, we will teach you how to navigate the virtual world while simultaneously using the latest-generation virtualization software. If you‽ve'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "generator(\n",
    "    \"In this course, we will teach you how to\",\n",
    "    max_length=30,\n",
    "    num_return_sequences=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8779db7e",
   "metadata": {},
   "source": [
    "### Mask filling <a name=\"mask-filling\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adb8cce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'This course will teach you all about mathematical models.',\n",
       "  'score': 0.19619831442832947,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical'},\n",
       " {'sequence': 'This course will teach you all about computational models.',\n",
       "  'score': 0.040527213364839554,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\")\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae436b6",
   "metadata": {},
   "source": [
    "### Named Entity Recognition <a name=\"named-entity-recognition\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9bad410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981693774461746,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019991238912,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932105541229248,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e48228",
   "metadata": {},
   "source": [
    "### Question Answering <a name=\"question-answering\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dba94e58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.6949764490127563, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answerer = pipeline(\"question-answering\")\n",
    "question_answerer(\n",
    "    question=\"Where do I work?\",\n",
    "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8c9711",
   "metadata": {},
   "source": [
    "### Summarisation <a name=\"summarisation\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bea63c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': ' America has changed dramatically during recent years . The number of engineering graduates in the U.S. has declined in traditional engineering disciplines such as mechanical, civil,    electrical, chemical, and aeronautical engineering . Rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number of \n",
    "    graduates in traditional engineering disciplines such as mechanical, civil, \n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of \n",
    "    the premier American universities engineering curricula now concentrate on \n",
    "    and encourage largely the study of engineering science. As a result, there \n",
    "    are declining offerings in engineering subjects dealing with infrastructure, \n",
    "    the environment, and related issues, and greater concentration on high \n",
    "    technology subjects, largely supporting increasingly complex scientific \n",
    "    developments. While the latter is important, it should not be at the expense \n",
    "    of more traditional engineering.\n",
    "\n",
    "    Rapidly developing economies such as China and India, as well as other \n",
    "    industrial countries in Europe and Asia, continue to encourage and advance \n",
    "    the teaching of engineering. Both China and India, respectively, graduate \n",
    "    six and eight times as many traditional engineers as does the United States. \n",
    "    Other industrial countries at minimum maintain their output, while America \n",
    "    suffers an increasingly serious decline in the number of engineering graduates \n",
    "    and a lack of well-educated engineers.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ae58f",
   "metadata": {},
   "source": [
    "### Translation <a name=\"translation\"></a>\n",
    "\n",
    "Models can be selected from https://huggingface.co/models\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9f8171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "translator(\"Ce cours est produit par Hugging Face.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b05e9ef",
   "metadata": {},
   "source": [
    "## Components of `pipeline` <a name=\"components\"></a>\n",
    "\n",
    "<img src=\"transformers-pipeline.png\" alt=\"components that form the pipeline\" width=\"80%\"/>\n",
    "\n",
    "Under the hood of a pipeline, there is a `tokenizer`, a `model` and some post-processing steps.\n",
    "\n",
    "First, text is passed to the tokenizer, then either `pytorch` or `tensorflow` tensors will be outputted.\\\n",
    "The tensors will be passed to the model and the model returns logits.\\\n",
    "Post-processing steps need to be done on the logits, e.g. passing them into a softmax layer to output variables that satisfies the NLP task.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09257c06",
   "metadata": {},
   "source": [
    "### Tokenizer <a name=\"tokenizer\"></a>\n",
    "\n",
    "A `tokenizer` takes in a text as input and does the following:\n",
    "\n",
    "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
    "- Mapping each token to an integer\n",
    "- Adding additional inputs that may be useful to the model\n",
    "\n",
    "To understand the different ways of tokenization, e.g. tokenizing by words, by characters, by symbols, check out: https://huggingface.co/course/chapter2/4?fw=pt\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a8c93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c7b7f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee79ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60de3b5f",
   "metadata": {},
   "source": [
    "The tokenizer can be saved by using the `pre-trained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5efc671d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('directory_on_my_computer/tokenizer_config.json',\n",
       " 'directory_on_my_computer/special_tokens_map.json',\n",
       " 'directory_on_my_computer/vocab.txt',\n",
       " 'directory_on_my_computer/added_tokens.json',\n",
       " 'directory_on_my_computer/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a57d1d2",
   "metadata": {},
   "source": [
    "**Under the hood of the tokenizer**\n",
    "\n",
    "The BERT tokenizer is a **subword** tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That’s the case here with `transformer`, which is split into two tokens: `transform` and `##er`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d19ecef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c52136e",
   "metadata": {},
   "source": [
    "#### Encoding <a name=\"encoding\"></a>\n",
    "\n",
    "The (subword) tokens are then turned into integers using the vocabulary.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e8f7df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 11303, 1200, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c8252",
   "metadata": {},
   "source": [
    "The tokens are stored as keys in the vocabulary dictionary and the corresponding index are values of the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c6259c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7993, 170, 11303, 1200, 2443, 1110, 3014]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.vocab[t] for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce9993",
   "metadata": {},
   "source": [
    "#### Decoding <a name=\"decoding\"></a>\n",
    "\n",
    "**Decoding** is going the other way around: from vocabulary indices, we want to get a string.\n",
    "\n",
    "This can be done with the `decode` method as follows:\n",
    "\n",
    "Note that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c255ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using a transformer network is simple\n"
     ]
    }
   ],
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5d24c2",
   "metadata": {},
   "source": [
    "### Padding <a name=\"padding\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c95669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4375de6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe43219d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a686bf2d",
   "metadata": {},
   "source": [
    "### Attention mask <a name=\"attention-mask\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e45b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63e9a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df701a90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "430a8322",
   "metadata": {},
   "source": [
    "### Truncation <a name=\"truncation\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c9cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e1f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dec897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f342a755",
   "metadata": {},
   "source": [
    "### `token_type_id` <a name=\"token-type-id\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb49608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69abd52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce65a771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1941a645",
   "metadata": {},
   "source": [
    "### Model <a name=\"model\"></a>\n",
    "\n",
    "Note that the checkpoint used should be the same as the one used by the tokenizer.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07b766f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing DistilBertModel: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73b74137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 16, 768])\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f438f1a9",
   "metadata": {},
   "source": [
    "Models for a specific task can be used, e.g. `AutoModelForSequenceClassification` instead of `AutoModel`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c35bd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03a61620",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\", \n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797be268",
   "metadata": {},
   "source": [
    "The inputs were 2 tokenized sentences and passed through an `AutoModelForSequenceClassification`, the logits are size `(2,2)` because the classification task classifies text into `2` classes and since there are 2 sentences, the size is `(2,2)`, much smaller than that of the `AutoModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477faa00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c6764c",
   "metadata": {},
   "source": [
    "### Postprocessing the output <a name=\"postprocessing\"></a>\n",
    "\n",
    "The logits don't really make sense, they don't sum to 1.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa89bf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.5607,  1.6123],\n",
      "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b06277b",
   "metadata": {},
   "source": [
    "The logits should be passed to a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3549ca3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0195e-02, 9.5980e-01],\n",
      "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69fecc",
   "metadata": {},
   "source": [
    "Now the predictions make more sense, `0.040195` for label `0` and `0.95980` label `1` for the first sentence.\\\n",
    "But we still don't know what the labels mean, to get the labels corresponding to each position, we can inspect the `id2label` attribute of the model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ca2467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9d1b98",
   "metadata": {},
   "source": [
    "The `AutoModel` class and all of its relatives are actually simple **wrappers** over the wide variety of models available in the library. It’s a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "\n",
    "However, if the type of model is known, the class that defines its architecture can be used directly.\\\n",
    "For example, for BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c1e7501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e48f8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.6.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce225a8",
   "metadata": {},
   "source": [
    "Doing the above creates a model from the default configuration and initializes it with random values. However for the above model to be usable, it needs to be trained, which is compute-resources intensive.\n",
    "\n",
    "Loading a Transformer model that is already trained is simple — we can do this using the `from_pretrained` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "904cfb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5752f95d",
   "metadata": {},
   "source": [
    "**Saving a model**\n",
    "\n",
    "A model can be saved by using the `save_pretrained` method. 2 files will be saved `'config.json'`, `'pytorch_model.bin'`.\n",
    "\n",
    "- `'config.json'`: contains information about the hyperparameters of the model, attributes necessary to build the model architecture and some metadata, e.g. the version of `Transformers` the model was saved in.\n",
    "- `'pytorch_model.bin'`: Model's weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "571a2276",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e0ba391",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer_config.json',\n",
       " 'special_tokens_map.json',\n",
       " 'config.json',\n",
       " 'tokenizer.json',\n",
       " 'vocab.txt',\n",
       " 'pytorch_model.bin']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"./directory_on_my_computer/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa73824",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "318ad07c",
   "metadata": {},
   "source": [
    "## Finetuning <a name=\"finetuning\"></a>\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f510da41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Same as before\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9e208413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is new\n",
    "batch[\"labels\"] = torch.tensor([1, 1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6976f12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
       "          2607,  2026,  2878,  2166,  1012,   102],\n",
       "        [  101,  2023,  2607,  2003,  6429,   999,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'labels': tensor([1, 1])}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cff6f5c",
   "metadata": {},
   "source": [
    "### MRPC Dataset\n",
    "\n",
    "The **MRPC (Microsoft Research Paraphrase Corpus)**  dataset, introduced in a paper by William B. Dolan and Chris Brockett consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing). It’s a small dataset, so it’s easy to experiment with training on it.\n",
    "\n",
    "The hub doesn't just contains model, it also contains datasets such as the MRPC dataset.\n",
    "\n",
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fa6e691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/Users/Tay/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'label', 'sentence1', 'sentence2'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'label', 'sentence1', 'sentence2'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'label', 'sentence1', 'sentence2'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecd77c9",
   "metadata": {},
   "source": [
    "The above code returns a `DatasetDict` object which contains the **training** set, the **validation** set, and the test set.\n",
    "\n",
    "Each of those contains several columns (`sentence1`, `sentence2`, `label`, and `idx`) and a variable number of rows, which are the number of elements in each set. They can be accessed like a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1289126d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'label': 1,\n",
       " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2572a5a3",
   "metadata": {},
   "source": [
    "The `'label': 1` entry does not say what does `1` refer to.\\\n",
    "To know which integer corresponds to which label, we can inspect the `features` of our `raw_train_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9b2b8fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': Value(dtype='int32', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402eb4a6",
   "metadata": {},
   "source": [
    "Behind the scenes, `label` is of type `ClassLabel`, and the mapping of integers to label name is stored in the names folder. `0` corresponds to `not_equivalent`, and `1` corresponds to `equivalent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d66e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ac4ec59",
   "metadata": {},
   "source": [
    "To preprocess the dataset, we need to convert the text to numbers the model can make sense of.\\\n",
    "We can tokenize them all at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2c855775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a133d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenizer(\n",
    "    raw_datasets[\"train\"][\"sentence1\"],\n",
    "    raw_datasets[\"train\"][\"sentence2\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9500d615",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069e0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd24c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d56a92c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4be16726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/Tay/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-45c1e2c429d5d127.arrow\n",
      "Loading cached processed dataset at /Users/Tay/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-2dc02d9648235839.arrow\n",
      "Loading cached processed dataset at /Users/Tay/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bddcfdfa2d4a83e8.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472434ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "83986f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068c08e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c934d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
